import torch

# Set seed for reproducibility
torch.manual_seed(42)

# Input (batch size 1, input size 3)
X = torch.tensor([[1.0, 2.0, 3.0]])     # Shape: (1, 3)

# Target output
y_true = torch.tensor([[1.0]])         # Shape: (1, 1)

# Element-wise exponents (instead of linear weights)
W1 = torch.tensor([[1.2, 0.5, 2.0]], requires_grad=False)  # (1, 3)
W2 = torch.tensor([[1.5, 2.0, 0.7]], requires_grad=False)  # (1, 3)

# Bias terms
b1 = torch.randn(1, 3)  # Bias after first layer
b2 = torch.randn(1, 1)  # Bias after second layer

# Hyperparameters
lr = 0.01
epochs = 200

for epoch in range(epochs):
    # ----- FORWARD -----
    
    # First transformation (custom element-wise function with exponent)
    z1 = torch.sign(X) * torch.abs(X) ** W1 + b1  # Shape: (1, 3)

    # Second transformation
    z2 = torch.sign(z1) * torch.abs(z1) ** W2 + b2  # Shape: (1, 3)

    # Mean over features to reduce to scalar output
    a2 = z2.mean(dim=1, keepdim=True)  # Shape: (1, 1)

    # Final activation (e.g., sigmoid)
    output = torch.sigmoid(a2)  # Shape: (1, 1)

    # ----- LOSS -----
    loss = torch.mean((output - y_true) ** 2)  # MSE loss

    # ----- BACKWARD -----
    # dL/doutput = 2 * (output - y_true)
    dL_doutput = 2 * (output - y_true)  # Shape: (1, 1)

    # doutput/da2 = sigmoid'(a2) = sigmoid(a2) * (1 - sigmoid(a2))
    da2 = output * (1 - output)  # Shape: (1, 1)

    # Chain rule: dL/da2
    dL_da2 = dL_doutput * da2  # Shape: (1, 1)

    # a2 = mean(z2), so dL/dz2 = dL/da2 / 3 (since mean over 3 features)
    dL_dz2 = dL_da2.expand_as(z2) / 3  # Shape: (1, 3)

    # dz2/dz1
    dz2_dz1 = W2 * torch.abs(z1) ** (W2 - 1) * torch.sign(z1)
    dL_dz1 = dL_dz2 * dz2_dz1  # Shape: (1, 3)

    # dz1/dX
    dz1_dX = W1 * torch.abs(X) ** (W1 - 1) * torch.sign(X)
    dL_dX = dL_dz1 * dz1_dX  # Shape: (1, 3)

    # ----- PARAMETER UPDATE -----
    b2 -= lr * dL_da2        # b2: (1, 1)
    b1 -= lr * dL_dz1        # b1: (1, 3)

    # ----- LOGGING -----
    if epoch % 20 == 0 or epoch == epochs - 1:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")